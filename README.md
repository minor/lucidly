# No Shot

**Master the art of prompting.** Like Leetcode, but for the age of AI.

No Shot is a competitive learning platform that benchmarks and gamifies your ability to prompt AI models to write code. Solve challenges, climb leaderboards, and sharpen your prompting skills.

ðŸ”— **Live at [noshot-ai.vercel.app](https://noshot-ai.vercel.app/)**

---

## Features

### âš¡ Arena Mode
- **10 challenges** across 5 categories (UI, function, debug, data, system) and 3 difficulty levels
- **Multi-model support** â€” GPT-5.2, GPT-5 Mini, GPT-5 Nano, Claude Opus 4.6, Claude Sonnet 4.5, Claude Haiku 4.5, Grok 4.1 Fast Reasoning, Grok Code Fast
- **Streaming chat** â€” Real-time LLM responses with live code generation
- **Live sandboxed preview** â€” UI challenges render in a Vercel Sandbox; function challenges run automated tests
- **ELO-style scoring** (0â€“1000) â€” Composite score based on accuracy (70%), speed (15%), and cost efficiency (15%)
- **Prompt feedback** â€” AI-generated tips to improve your prompting technique
- **Leaderboards** â€” Per-challenge rankings with sortable metrics

### ðŸ¤– Agent Benchmarks
- Run autonomous AI agents (Claude Agent SDK, OpenAI Assistant, and more) against any challenge
- Watch agent thinking traces update live in the UI

### ðŸ“‹ Interview Mode
- Create custom interview rooms with coding, frontend, and system design challenges
- Share invite codes with candidates
- Observe candidates solving challenges in real time
- View detailed post-interview reports with turn-by-turn transcripts and metrics

---

## Tech Stack

| Layer | Technologies |
| --- | --- |
| **Frontend** | Next.js 16, React 19, TypeScript, Tailwind CSS 4, Zustand, Auth0 |
| **Backend** | Python 3.11+, FastAPI, Uvicorn, Pydantic |
| **LLM Providers** | OpenAI, Anthropic, xAI (Grok) |
| **Database** | Supabase (PostgreSQL) |
| **Sandboxing** | Vercel Sandbox (UI), Modal (agents) |
| **Testing** | Playwright (E2E) |

---

## Prerequisites

- **[Bun](https://bun.sh)** (recommended) or Node.js 18+
- **Python 3.11+**
- **[uv](https://docs.astral.sh/uv/)** â€” Python package manager
  ```bash
  curl -LsSf https://astral.sh/uv/install.sh | sh
  ```

---

## Getting Started

### 1. Install root dependencies

From the project root:

```bash
bun install
```

This installs `concurrently` to run both servers with a single command.

### 2. Install frontend dependencies

```bash
cd frontend
bun install
```

### 3. Install backend dependencies

```bash
cd backend
uv sync
```

### 4. Configure API keys

Create a `backend/.env` file with your LLM provider keys. **At minimum, you need one provider:**

```env
# â”€â”€ Required (at least one) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# OpenAI (powers GPT models)
OPENAI_API_KEY=sk-...
# OPENAI_BASE_URL=https://api.openai.com/v1        # default; change for OpenRouter etc.

# Anthropic (powers Claude models)
ANTHROPIC_API_KEY=sk-ant-...

# xAI (powers Grok models)
XAI_API_KEY=xai-...
```

**Where to get keys:**
- OpenAI â†’ [platform.openai.com/api-keys](https://platform.openai.com/api-keys)
- Anthropic â†’ [console.anthropic.com](https://console.anthropic.com/)
- xAI â†’ [console.x.ai](https://console.x.ai/)

<details>
<summary><strong>Optional environment variables</strong></summary>

```env
# â”€â”€ Database â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=eyJ...

# â”€â”€ Auth â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Set these in frontend/.env.local
# NEXT_PUBLIC_AUTH0_DOMAIN=your-tenant.auth0.com
# NEXT_PUBLIC_AUTH0_CLIENT_ID=...

# â”€â”€ Agent benchmarks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
USE_INPROCESS_AGENT=true               # default; runs agent inside backend process
AGENT_INTERNAL_SECRET=                  # only needed for Modal-based agents
BACKEND_PUBLIC_URL=http://localhost:8000

# â”€â”€ Modal (cloud agent execution) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MODAL_TOKEN_ID=...
# MODAL_TOKEN_SECRET=...

# â”€â”€ Browserbase / Stagehand (agent web scraping) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# BROWSERBASE_API_KEY=...
# BROWSERBASE_PROJECT_ID=...

# â”€â”€ Server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HOST=0.0.0.0
# PORT=8000
# CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000
```

</details>

### 5. Start the dev server

From the project root:

```bash
bun run dev
```

This starts both servers concurrently:

| Service | URL |
| --- | --- |
| Backend (FastAPI) | `http://localhost:8000` |
| Frontend (Next.js) | `http://localhost:3000` |

---

## Project Structure

```
lucidly/
â”œâ”€â”€ frontend/               # Next.js app (see frontend/README.md)
â”‚   â”œâ”€â”€ app/                # App Router pages & API routes
â”‚   â”œâ”€â”€ components/         # Shared React components
â”‚   â”œâ”€â”€ lib/                # API client, types, utilities
â”‚   â””â”€â”€ hooks/              # Custom React hooks
â”œâ”€â”€ backend/                # FastAPI server
â”‚   â”œâ”€â”€ main.py             # App entry point & API routes
â”‚   â”œâ”€â”€ config.py           # Settings & model pricing
â”‚   â”œâ”€â”€ llm.py              # LLM client (OpenAI-compatible + Anthropic + xAI)
â”‚   â”œâ”€â”€ challenges.py       # Challenge loader
â”‚   â”œâ”€â”€ challenges.json     # Challenge definitions
â”‚   â”œâ”€â”€ sessions.py         # Session & leaderboard management
â”‚   â”œâ”€â”€ agents.py           # Agent definitions
â”‚   â”œâ”€â”€ agent_runner.py     # Agent execution loop
â”‚   â”œâ”€â”€ sandbox.py          # Code sandbox execution
â”‚   â”œâ”€â”€ evaluation/         # Scoring engine, test runner, evaluator
â”‚   â””â”€â”€ interviews/         # Interview mode (rooms, sessions, realtime)
â”œâ”€â”€ modal_agent/            # Modal cloud agent deployment
â”‚   â””â”€â”€ app.py
â””â”€â”€ package.json            # Root scripts (concurrently)
```

---

## Agent Benchmarks (Optional)

The **Agents** page lets you run autonomous AI agents against challenges and watch them work.

### In-process (default)

No extra setup needed. The agent runs inside the backend process. Just open **Agents**, pick an agent and challenge, and click **Run**.

### With Modal (cloud)

For cloud-based agent execution:

1. Authenticate: `modal token set`
2. Deploy: `cd modal_agent && modal deploy app.py`
3. In `backend/.env`:
   ```env
   USE_INPROCESS_AGENT=false
   BACKEND_PUBLIC_URL=<url-reachable-from-Modal>   # e.g. ngrok URL
   AGENT_INTERNAL_SECRET=<shared-secret>
   ```

### Supported agents

| Agent | SDK | Required Key |
| --- | --- | --- |
| Claude Agent SDK | `claude-agent-sdk` | `ANTHROPIC_API_KEY` |
| OpenAI Assistant | OpenAI Assistants API | `OPENAI_API_KEY` |

> **Tip:** Tail the debug log for agent traces: `tail -f .cursor/debug.log`

---

## License

Private â€” all rights reserved.
